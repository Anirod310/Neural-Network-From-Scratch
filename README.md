# Neural-network-from-scratch
A project in which I learn to code a L-layer neural network from scratch, using numpy and matplotlib. The project is separated in different stages : 
- Code a simple L-layer neural network with basic parameters, hyperparameters and gradient descent implementation, without any regularisation or optimisation method :
    - Try different parameter initialisations.
    - Try different activation functions.
    - Vary the size of the neural network (adding/removing layers/nodes).
    - Analyze the results for each case.

- Improve the model by adding regularization and gradient checking :
    - Try L2 regularization and analyze the results.
    - Implement gradient checking to verify the accuracy of the backprob part of the model.

- Optimize the model even more by implementing new algorithms and hyperparameters :
    - Implement mini-batch gradient descent and compare the performances
    - Implement momentum
    - Implement Adam


You can follow the entire project walkthrough in the [report](REPORT.md) file.
## Environment

To ensure reproducibility, the following environment setup is recommended:

- **Python version**: 3.8 or later
- **IDE**: Jupyter Notebook or any other Python IDE
## Dependencies

Install the required dependencies using the following command:

```bash
pip install -r requirements.txt
```
## Troubleshooting  

If you are having trouble getting the model to work, you can check the following :
- Make sure that you installed the correct dependencies.
- Make sure that the data are formated correctly.
If you still have trouble, you can ask for help on a forum or chat room dedicated to linear regression, or contact me at **bousek.dorian@gmail.com**.  

I hope this helps ! 
