# Neural-network-from-scratch
A project in which I learn to code a L-layer neural network from scratch, using numpy and matplotlib. The project is separated in different stages : 
- Code a simple L-layer neural network with basic parameters, hyperparameters and gradient descent implementation, without any regularisation or optimisation method :
    - Try different parameter initialisations.
    - Try different activation functions.
    - Vary the size of the neural network (adding/removing layers/nodes).
    - Analyze the results for each case.

- Improve the model by adding regularization and gradient checking :
    - Try L2 regularization and dropout and analyze the results for each.
    - Implement gradient checking to verify the accuracy of the backprob part of the model.

- Optimize the model even more by implementing new algorithms and hyperparameters :
    - Implement mini-batch gradient descent and compare the performances
    - Implement momentum
    - Implement Adam
    - Implement learning rate decay


You can follow the entire project walkthrough in the [report](REPORT.md) file.
